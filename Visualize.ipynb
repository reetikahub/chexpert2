{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1c2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import timm\n",
    "from functools import partial\n",
    "\n",
    "   \n",
    "from PIL import Image\n",
    "import sys\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fe8f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXpert-v1.0-small\n"
     ]
    }
   ],
   "source": [
    "# dataset and models\n",
    "from dataset import ChexpertSmall, extract_patient_ids\n",
    "from torchvision.models import densenet121, resnet152\n",
    "#from models.efficientnet import construct_model\n",
    "#from models.attn_aug_conv import DenseNet, ResNet, Bottleneck\n",
    "from vit_pytorch import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fa53e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\r\n",
      "Requirement already satisfied: tensorboardX in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (2.5)\r\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboardX) (1.15.0)\r\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboardX) (1.19.2)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboardX) (3.17.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb7b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(model, x, hooks, cls_idx=None):\n",
    "    \"\"\" cf CheXpert: Test Results / Visualization; visualize final conv layer, using grads of final linear layer as weights,\n",
    "    and performing a weighted sum of the final feature maps using those weights.\n",
    "    cf Grad-CAM https://arxiv.org/pdf/1610.02391.pdf \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # register backward hooks\n",
    "    conv_features, linear_grad = [], []\n",
    "    forward_handle = hooks['forward'].register_forward_hook(lambda module, in_tensor, out_tensor: conv_features.append(out_tensor))\n",
    "    backward_handle = hooks['backward'].register_backward_hook(lambda module, grad_input, grad_output: linear_grad.append(grad_input))\n",
    "\n",
    "    # run model forward and create a one hot output for the given cls_idx or max class\n",
    "    outputs = model(x)\n",
    "    if not cls_idx: cls_idx = outputs.argmax(1)\n",
    "    one_hot = F.one_hot(cls_idx, outputs.shape[1]).float().requires_grad_(True)\n",
    "\n",
    "    # run model backward\n",
    "    one_hot.mul(outputs).sum().backward()\n",
    "\n",
    "    # compute weights; cf. Grad-CAM eq 1 -- gradients flowing back are global-avg-pooled to obtain the neuron importance weights\n",
    "    weights = linear_grad[0][2].mean(1).view(1, -1, 1, 1)\n",
    "    # compute weighted combination of forward activation maps; cf Grad-CAM eq 2; linear combination over channels\n",
    "    cam = F.relu(torch.sum(weights * conv_features[0], dim=1, keepdim=True))\n",
    "\n",
    "    # normalize each image in the minibatch to [0,1] and upscale to input image size\n",
    "    cam = cam.clone()  # avoid modifying tensor in-place\n",
    "    with torch.no_grad():\n",
    "        def norm_ip(t, min, max):\n",
    "            t.clamp_(min=min, max=max)\n",
    "            t.add_(-min).div_(max - min + 1e-5)\n",
    "\n",
    "        for t in cam:  # loop over mini-batch dim\n",
    "            norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "    cam = F.interpolate(cam, x.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "    # cleanup\n",
    "    forward_handle.remove()\n",
    "    backward_handle.remove()\n",
    "    model.zero_grad()\n",
    "\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbf85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grad_cam(model, dataloader, grad_cam_hooks, output_dir):\n",
    "    attr_names = dataloader.dataset.attr_names\n",
    "    # 1. run through model to compute logits and grad-cam\n",
    "    imgs, labels, scores, masks, idxs = [], [], [], [], []\n",
    "    for x, target, idx in dataloader:\n",
    "        imgs += [x]\n",
    "        labels += [target]\n",
    "        idxs += idx.tolist()\n",
    "        x = x.to(device)\n",
    "        scores += [model(x).cpu()]\n",
    "        masks  += [grad_cam(model, x, grad_cam_hooks).cpu()]\n",
    "    imgs, labels, scores, masks = torch.cat(imgs), torch.cat(labels), torch.cat(scores), torch.cat(masks)\n",
    "\n",
    "    # 2. renormalize images and convert everything to numpy for matplotlib\n",
    "    imgs.mul_(0.0349).add_(0.5330)\n",
    "    imgs = imgs.permute(0,2,3,1).data.numpy()\n",
    "    labels = labels.data.numpy()\n",
    "    patient_ids = extract_patient_ids(dataloader.dataset, idxs)\n",
    "    masks = masks.permute(0,2,3,1).data.numpy()\n",
    "    probs = scores.sigmoid().data.numpy()\n",
    "\n",
    "    # 3. make column grid of [model probs table, original image, grad-cam image] for each attr + other categories\n",
    "    for attr, vis_idxs in zip(dataloader.dataset.vis_attrs, dataloader.dataset.vis_idxs):\n",
    "        fig, axs = plt.subplots(3, 3, figsize=(4 * imgs.shape[1]/100, 3.3 * imgs.shape[2]/100), dpi=100, frameon=False)\n",
    "        fig.suptitle(attr)\n",
    "        for i, idx in enumerate(vis_idxs):\n",
    "            offset = idxs.index(idx)\n",
    "            visualize_one(model, imgs[offset], masks[offset], labels[offset], patient_ids[offset], probs[offset], attr_names, axs[i], output_dir)\n",
    "\n",
    "        filename = 'vis_{}_step_{}.png'.format(attr.replace(' ', '_'), 100)\n",
    "        plt.savefig(os.path.join(output_dir, 'vis', filename), dpi=100)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacaee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_att_rollout(model, dataloader, output_dir):\n",
    "    attr_names = dataloader.dataset.attr_names\n",
    "    # 1. run through model to compute logits and grad-cam\n",
    "    imgs, labels, scores, masks, idxs = [], [], [], [], []\n",
    "    for x, target, idx in dataloader:\n",
    "        imgs += [x]\n",
    "        labels += [target]\n",
    "        idxs += idx.tolist()\n",
    "        x = x.to(device)\n",
    "        scores += [model(x).cpu()]\n",
    "        attention_rollout = VITAttentionRollout(model, head_fusion='max', discard_ratio=0.9)\n",
    "        masks += [attention_rollout(x)]\n",
    "    imgs, labels, scores, masks = torch.cat(imgs), torch.cat(labels), torch.cat(scores), torch.cat(masks)\n",
    "\n",
    "    # 2. renormalize images and convert everything to numpy for matplotlib\n",
    "    imgs.mul_(0.0349).add_(0.5330)\n",
    "    imgs = imgs.permute(0,2,3,1).data.numpy()\n",
    "    labels = labels.data.numpy()\n",
    "    patient_ids = extract_patient_ids(dataloader.dataset, idxs)\n",
    "    masks = masks.permute(0,2,3,1).data.numpy()\n",
    "    probs = scores.sigmoid().data.numpy()\n",
    "\n",
    "    # 3. make column grid of [model probs table, original image, grad-cam image] for each attr + other categories\n",
    "    for attr, vis_idxs in zip(dataloader.dataset.vis_attrs, dataloader.dataset.vis_idxs):\n",
    "        fig, axs = plt.subplots(3, 3, figsize=(4 * imgs.shape[1]/100, 3.3 * imgs.shape[2]/100), dpi=100, frameon=False)\n",
    "        fig.suptitle(attr)\n",
    "        for i, idx in enumerate(vis_idxs):\n",
    "            offset = idxs.index(idx)\n",
    "            visualize_one(model, imgs[offset], masks[offset], labels[offset], patient_ids[offset], probs[offset], attr_names, axs[i], output_dir)\n",
    "\n",
    "        filename = 'vis_{}_step_{}.png'.format(attr.replace(' ', '_'), 100)\n",
    "        plt.savefig(os.path.join(output_dir, 'vis', filename), dpi=100)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f4dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_one(model, img, mask, label, patient_id, prob, attr_names, axs, output_dir):\n",
    "    \"\"\" display [table of model vs ground truth probs | original image | grad-cam mask image] in a given suplot axs \"\"\"\n",
    "    # sort data by prob high to low\n",
    "    sort_idxs = prob.argsort()[::-1]\n",
    "    label = label[sort_idxs]\n",
    "    prob = prob[sort_idxs]\n",
    "    names = [attr_names[i] for i in sort_idxs]\n",
    "    # 1. left -- show table of ground truth and predictions, sorted by pred prob high to low\n",
    "    axs[0].set_title(patient_id)\n",
    "    data = np.stack([label, prob.round(3)]).T\n",
    "    axs[0].table(cellText=data, rowLabels=names, colLabels=['Ground truth', 'Pred. prob'],\n",
    "                 rowColours=plt.cm.Greens(0.5*label),\n",
    "                 cellColours=plt.cm.Greens(0.5*data), cellLoc='center', loc='center')\n",
    "    axs[0].axis('tight')\n",
    "    # 2. middle -- show original image\n",
    "    axs[1].set_title('Original image', fontsize=10)\n",
    "    axs[1].imshow(img.squeeze(), cmap='gray')\n",
    "    # 3. right -- show heatmap over original image with predictions\n",
    "    axs[2].set_title('Top class activation \\n{}: {:.4f}'.format(names[0], prob[0]), fontsize=10)\n",
    "    axs[2].imshow(img.squeeze(), cmap='gray')\n",
    "    axs[2].imshow(mask.squeeze(), cmap='jet', alpha=0.5)\n",
    "\n",
    "    for ax in axs: ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04882ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_attn(x, patient_ids, idxs, attn_layers, output_dir, batch_element=0):\n",
    "    H, W = x.shape[2:]\n",
    "    nh = attn_layers[0].nh\n",
    "\n",
    "    # select which pixels to visualize -- e.g. select virtices of a center square of side 1/3 of the image dims\n",
    "    pix_to_vis = lambda h, w: [(h//3, w//3), (h//3, int(2*w/3)), (int(2*h/3), w//3), (int(2*h/3), int(2*w/3))]\n",
    "    window = 30  # take mean attn around the pix_to_vis in a window of size ws\n",
    "\n",
    "    for j, l in enumerate(attn_layers):\n",
    "        # visualize attention maps (rows for each head; columns for each pixel)\n",
    "        fig, axs = plt.subplots(nh+1, 4, figsize=(3,3/4*(1+nh)), frameon=False)\n",
    "        fig.suptitle(patient_ids[batch_element], fontsize=8)\n",
    "        # display target image; highlight pixel\n",
    "        for ax, (ph, pw) in zip(axs[0], pix_to_vis(H,W)):\n",
    "            image = x.clone().detach().mul_(0.0349).add_(0.5330)  # renormalize\n",
    "            image[:,:,ph-window:ph+window,pw-window:pw+window] = torch.tensor([1., 215/255, 0]).view(1,3,1,1)   # add yellow pixel on the pix_to_vis for visualization\n",
    "            ax.imshow(image[batch_element].permute(1,2,0).numpy())\n",
    "            ax.axis('off')\n",
    "        # display attention maps\n",
    "        # get attention weights tensor for the batch element\n",
    "        attn = l.weights.data[batch_element]\n",
    "        # reshape attn tensor and select the pixels to visualize\n",
    "        h = w = int(np.sqrt(attn.shape[-1]))\n",
    "        ws = max(1, int(window * h/H))  # scale window to feature map size\n",
    "        attn = attn.reshape(nh, h, w, h, w)\n",
    "        for i, (ph, pw) in enumerate(pix_to_vis(h,w)):\n",
    "            for h in range(nh):\n",
    "                axs[h+1, i].imshow(attn[h, ph-ws:ph+ws, pw-ws:pw+ws, :, :].mean([0,1]).cpu().numpy())\n",
    "                axs[h+1, i].axis('off')\n",
    "\n",
    "\n",
    "        filename = 'attn_image_idx_{}_{}_layer_{}.png'.format(idxs[batch_element], batch_element, j)\n",
    "        fig.subplots_adjust(0,0,1,0.95,0.05,0.05)\n",
    "        plt.savefig(os.path.join(output_dir, 'vis', filename))\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4adb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataloader(resize, mode, mini_data):\n",
    "    assert mode in ['train', 'valid', 'vis']\n",
    "    data_path = '../'\n",
    "    batch_size = 1\n",
    "    transforms = T.Compose([\n",
    "        T.Resize(resize) if resize else T.Lambda(lambda x: x),\n",
    "        T.CenterCrop(320 if not resize else resize),\n",
    "        lambda x: torch.from_numpy(np.array(x, copy=True)).float().div(255).unsqueeze(0),   # tensor in [0,1]\n",
    "        T.Normalize(mean=[0.5330], std=[0.0349]),                                           # whiten with dataset mean and st\n",
    "        lambda x: x.expand(3,-1,-1)\n",
    "#        T.Resize((args.resize, args.resize)),\n",
    "#        T.RandomHorizontalFlip(),\n",
    "#        T.ToTensor(),\n",
    "        ])                                                       # expand to 3 channels\n",
    "\n",
    "    dataset = ChexpertSmall(data_path, mode, transforms, mini_data)\n",
    "\n",
    "    return DataLoader(dataset, batch_size, shuffle=(mode=='train'), pin_memory=(device.type=='cuda'),\n",
    "                      num_workers=0 if mode=='valid' else 16)  # since evaluating the valid_dataloader is called inside the\n",
    "                                                              # train_dataloader loop, 0 workers for valid_dataloader avoids\n",
    "                                                              # forking (cf torch dataloader docs); else memory sharing gets clunky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c3f50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "../CheXpert-v1.0-small/valid.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(ChexpertSmall.attr_names)\n",
    "print(n_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vis_dataloader = fetch_dataloader(resize=224, mode='vis', mini_data= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b13183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet121().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dec80b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Linear(model.classifier.in_features, out_features=n_classes).to(device)\n",
    "grad_cam_hooks = {'forward': model.features, 'backward': model.classifier}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "676049de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "output_dir = './results/'\n",
    "visualize_grad_cam(model, vis_dataloader, grad_cam_hooks, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a602d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(attentions, discard_ratio, head_fusion):\n",
    "    print(attentions)\n",
    "    result = torch.eye(attentions[0].size(-1))\n",
    "    with torch.no_grad():\n",
    "        for attention in attentions:\n",
    "            if head_fusion == \"mean\":\n",
    "                attention_heads_fused = attention.mean(axis=1)\n",
    "            elif head_fusion == \"max\":\n",
    "                attention_heads_fused = attention.max(axis=1)[0]\n",
    "            elif head_fusion == \"min\":\n",
    "                attention_heads_fused = attention.min(axis=1)[0]\n",
    "            else:\n",
    "                raise \"Attention head fusion type Not supported\"\n",
    "\n",
    "            # Drop the lowest attentions, but\n",
    "            # don't drop the class token\n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
    "            indices = indices[indices != 0]\n",
    "            flat[0, indices] = 0\n",
    "\n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0*I)/2\n",
    "            a = a / a.sum(dim=-1)\n",
    "\n",
    "            result = torch.matmul(a, result)\n",
    "\n",
    "    # Look at the total attention between the class token,\n",
    "    # and the image patches\n",
    "    mask = result[0, 0 , 1 :]\n",
    "    # In case of 224x224 image, this brings us from 196 to 14\n",
    "    width = int(mask.size(-1)**0.5)\n",
    "    mask = mask.reshape(width, width)\n",
    "    mask = mask / torch.max(mask)\n",
    "    return mask    \n",
    "\n",
    "class VITAttentionRollout:\n",
    "    def __init__(self, model, attention_layer_name='attend', head_fusion=\"mean\",\n",
    "        discard_ratio=0.9):\n",
    "        self.model = model.to(device)\n",
    "        self.head_fusion = head_fusion\n",
    "        self.discard_ratio = discard_ratio\n",
    "        for name, module in self.model.named_modules():\n",
    "            if attention_layer_name in name:\n",
    "                module.register_forward_hook(self.get_attention)\n",
    "\n",
    "        self.attentions = []\n",
    "\n",
    "    def get_attention(self, module, input, output):\n",
    "        self.attentions.append(output.cpu())\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        self.attentions = []\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor.to(device))\n",
    "\n",
    "        return rollout(self.attentions, self.discard_ratio, self.head_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd780420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_rollout(attentions, gradients, discard_ratio):\n",
    "    result = torch.eye(attentions[0].size(-1))\n",
    "    with torch.no_grad():\n",
    "        for attention, grad in zip(attentions, gradients):                \n",
    "            weights = grad\n",
    "            attention_heads_fused = (attention*weights).mean(axis=1)\n",
    "            attention_heads_fused[attention_heads_fused < 0] = 0\n",
    "\n",
    "            # Drop the lowest attentions, but\n",
    "            # don't drop the class token\n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
    "            #indices = indices[indices != 0]\n",
    "            flat[0, indices] = 0\n",
    "\n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0*I)/2\n",
    "            a = a / a.sum(dim=-1)\n",
    "            result = torch.matmul(a, result)\n",
    "    \n",
    "    # Look at the total attention between the class token,\n",
    "    # and the image patches\n",
    "    mask = result[0, 0 , 1 :]\n",
    "    # In case of 224x224 image, this brings us from 196 to 14\n",
    "    width = int(mask.size(-1)**0.5)\n",
    "    mask = mask.reshape(width, width).numpy()\n",
    "    mask = mask / np.max(mask)\n",
    "    return mask    \n",
    "\n",
    "class VITAttentionGradRollout:\n",
    "    def __init__(self, model, attention_layer_name='attend', discard_ratio=0.9):\n",
    "        self.model = model\n",
    "        self.discard_ratio = discard_ratio\n",
    "        for name, module in self.model.named_modules():\n",
    "            if attention_layer_name in name:\n",
    "                module.register_forward_hook(self.get_attention)\n",
    "                module.register_backward_hook(self.get_attention_gradient)\n",
    "\n",
    "        self.attentions = []\n",
    "        self.attention_gradients = []\n",
    "\n",
    "    def get_attention(self, module, input, output):\n",
    "        self.attentions.append(output.cpu())\n",
    "\n",
    "    def get_attention_gradient(self, module, grad_input, grad_output):\n",
    "        self.attention_gradients.append(grad_input[0].cpu())\n",
    "\n",
    "    def __call__(self, input_tensor, category_index):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        category_mask = torch.zeros(output.size())\n",
    "        category_mask[:, category_index] = 1\n",
    "        loss = (output*category_mask).sum()\n",
    "        loss.backward()\n",
    "\n",
    "        return grad_rollout(self.attentions, self.attention_gradients,\n",
    "            self.discard_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de913052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataloader(resize, mode, mini_data):\n",
    "    assert mode in ['train', 'valid', 'vis']\n",
    "    data_path = '../'\n",
    "    batch_size = 16\n",
    "    transforms = T.Compose([\n",
    "        T.Resize(resize) if resize else T.Lambda(lambda x: x),\n",
    "        T.CenterCrop(320 if not resize else resize),\n",
    "        lambda x: torch.from_numpy(np.array(x, copy=True)).float().div(255).unsqueeze(0),   # tensor in [0,1]\n",
    "        T.Normalize(mean=[0.5330], std=[0.0349]),                                           # whiten with dataset mean and st\n",
    "#         lambda x: x.expand(3,-1,-1)\n",
    "#        T.Resize((args.resize, args.resize)),\n",
    "#        T.RandomHorizontalFlip(),\n",
    "#        T.ToTensor(),\n",
    "        ])                                                       # expand to 3 channels\n",
    "\n",
    "    dataset = ChexpertSmall(data_path, mode, transforms, mini_data)\n",
    "\n",
    "    return DataLoader(dataset, batch_size, shuffle=(mode=='train'), pin_memory=(device.type=='cuda')\n",
    "                      num_workers=0 if mode=='valid' else 16)  # since evaluating the valid_dataloader is called inside the\n",
    "                                                              # train_dataloader loop, 0 workers for valid_dataloader avoids\n",
    "                                                              # forking (cf torch dataloader docs); else memory sharing gets clunky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d044cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "cuda\n",
      "../CheXpert-v1.0-small/valid.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(ChexpertSmall.attr_names)\n",
    "print(n_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "vis_dataloader = fetch_dataloader(resize=256, mode='vis', mini_data= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecd308f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5c8f25b20bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvisualize_att_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-d081080bfdd5>\u001b[0m in \u001b[0;36mvisualize_att_rollout\u001b[0;34m(model, dataloader, output_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 1. run through model to compute logits and grad-cam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "output_dir = './results/2022-05-27_02-52-52'\n",
    "model = ViT(image_size = 256, patch_size = 16, num_classes = 5, dim = 512, depth = 6, heads = 8, channels = 1, mlp_dim = 1024).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "scheduler = None\n",
    "visualize_att_rollout(model, vis_dataloader, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b880f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transformer.layers[0][0].fn.attend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20b51368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_hooks():\n",
    "    def __init__(self, model, attention_layer_name='attend'):\n",
    "        self.model = model\n",
    "        for name, module in self.model.named_modules():\n",
    "            if attention_layer_name in name:\n",
    "                module.register_forward_hook(self.get_attention)\n",
    "\n",
    "        self.attentions = []\n",
    "\n",
    "    def get_attention(self, module, input, output):\n",
    "        self.attentions.append(output.cpu())\n",
    "\n",
    "    def __call__(self):\n",
    "        self.attentions = []\n",
    "        return self.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dc0a753",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-fd180a3088c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattn_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvis_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpatient_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_patient_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# visualize stored attention weights for each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "attn_hooks = attention_hooks(model)\n",
    "for x, _, idxs in vis_dataloader:\n",
    "    model(x.to(device))\n",
    "    patient_ids = extract_patient_ids(vis_dataloader.dataset, idxs)\n",
    "    # visualize stored attention weights for each image\n",
    "    print(attn_hooks())\n",
    "    for i in range(len(x)): vis_attn(x, patient_ids, idxs, attn_hooks(), args, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "310e0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c9e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
